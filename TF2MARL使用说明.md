# 使用说明

## 0 训练目的

对`maddpg, matd3, mad3pg`三种策略进行交叉对比。

使用特殊符号（如`- [ ]`和`- [x]`）来表示未完成和已完成的任务。以下是一个示例表格，其中包括了`good_policy`和`adv_policy`的组合，以及训练进度的表示：

下面是更新后的表格：

|         | **Good Policy** | **maddpg** | **matd3** | **mad3pg** |
|---------|-----------------|------------|-----------|------------|
| **Adv Policy** | **maddpg**      | 0 ☐ 1 ☐ 2 ☐ 3 ☐ | 0 ☐ 1 ☐ 2 ☐ 3 ☐ | 0 ☐ 1 ☐ 2 ☐ 3 ☐ |
|         | **matd3**       | 0 ☐ 1 ☐ 2 ☐ 3 ☐ | 0 ☐ 1 ☐ 2 ☐ 3 ☐ | 0 ☐ 1 ☐ 2 ☐ 3 ☐ |
|         | **mad3pg**      | 0 ☐ 1 ☐ 2 ☐ 3 ☐ | 0 ☐ 1 ☐ 2 ☐ 3 ☐ | 0 ☐ 1 ☐ 2 ☐ 3 ☐ |

在这个表格中，横轴代表`good_policy`的选择，而纵轴代表`adv_policy`的选择。表格中的每个单元格包含四个部分，分别对应着训练进度的四个阶段（0, 1, 2, 3）。☐ 表示该阶段未完成，而 ☑️ 表示该阶段已完成。您可以根据实际进度手动更新这些符号。

例如，如果您完成了`good_policy`为`maddpg`和`adv_policy`为`matd3`的训练（0阶段），则相应单元格应更新为“0 ☑️ 1 ☐ 2 ☐ 3 ☐”。



## 1 基本使用

### 启动软件
1. **打开终端**：首先打开您的Linux终端。
2. **导航到工作目录**：使用`cd`命令导航到对应的目录。
   ```bash
   cd /app/tf2marl_1
   ```
   替换`/app/tf2marl_1`为项目所在的路径，docker导入的话应该是对着的。

3. **激活Conda环境**：在终端中输入以下命令来激活Conda环境。
   ```bash
   conda activate nnn
   ```
   确保您已经安装了Conda并创建了名为`nnn`的环境，docker导入的话应该是对着的。。

4. **运行主程序**：执行以下命令来启动主程序`train.py`。
   ```bash
   python train.py
   ```

### 交互与操作
在运行`train.py`后，您将看到如下提示信息：
```
input val[0 -> train, 1 -> add_learning, 2 -> display, 3 -> evaluate]:
```
这是程序等待您输入的信号。根据需要执行的操作，输入相应的数字并按回车键。具体操作如下所述。

## 2 功能详解

### 0 -> 训练（Train）
- **操作**：输入`0`并按回车。
- **描述**：此操作会启动一个新的模型训练过程。如果在存储路径下没有子文件夹，程序会自动在`/1/models`创建一个新的目录来存储模型文件。

### 1 -> 增加学习（Add Learning）
- **操作**：输入`1`并按回车。
- **描述**：此操作基于已有模型进行额外的训练。当存储路径中存在`/1/`文件夹时，程序会在`/2/models`创建一个新目录来存储更新后的模型文件。

### 2 -> 展示（Display）
- **操作**：输入`2`并按回车。
- **描述**：此操作会演示已有模型的行为。程序会在相应模型目录下创建一个`/demo/1/`的子目录来存储演示过程。

### 3 -> 评估（Evaluate）
- **操作**：输入`3`并按回车。
- **描述**：此操作会测试并评估已有模型的性能。评估结果将存储在对应的模型目录中。

## 3 如何切换策略组合

在多智能体强化学习环境中，不同的算法组合可以用来测试智能体在各种情境下的表现。本文档描述了如何在`train.py`脚本中切换不同的策略组合，以及如何配置相应的参数。

### 关键代码段

在`train.py`中，智能体的策略由下列代码段定义（96行到98行）：

```python
# Agent Parameters
good_policy = "maddpg"  # policy of "good" agents in env
adv_policy = 'matd3'    # policy of adversary agents in env
```

### 修改策略

要修改智能体的策略，您需要根据需求更改`good_policy`和`adv_policy`变量的值。以下是可用策略及其对应的操作说明：

#### 1 可用策略
- `maddpg`: Multi-Agent Deep Deterministic Policy Gradient
- `matd3`: Multi-Agent Twin Delayed DDPG
- `mad3pg`: 另一个多智能体DDPG变体

### 操作步骤

1. **打开`train.py`**:
   找到并打开`train.py`脚本文件。

2. **定位策略设置代码段**:
   定位到定义`good_policy`和`adv_policy`的代码段（96行到98行）。

3. **修改智能体策略**:
   根据需要更改这些变量的值。例如，要将好的智能体策略设置为`maddpg`，并将对立方智能体策略设置为`mad3pg`，您需要更改代码如下：

   ```python
   good_policy = "maddpg"
   adv_policy = "mad3pg"
   ```

4. **保存并关闭文件**:
   保存您所做的更改，并关闭文件。

5. **重新运行脚本**:
   重新运行`train.py`以应用新的策略设置。


### 文件管理

在处理`train.py`脚本中的第49行到51行这些代码时，您需要注意以下几个关键点，尤其是在训练模型时：

### 代码段

```python
# 当前的场景
scenario = 'stage2'  # 可根据需要修改此处

# 加载目录
load_dir = f"results/{scenario}/10/models"  # 示例路径，根据需要调整

# 保存目录
save_dir = f"results/{scenario}"  # 保存新模型的目录
```

### 注意事项

1. **场景设置 (`scenario`):**
   - `scenario`变量定义了训练的具体场景。在这个例子中，它被设置为`'stage2'`。
   - 确保`scenario`值正确反映了您要进行训练的具体场景或任务。不同的场景可能意味着不同的环境设置、目标、或挑战。

2. **加载目录 (`load_dir`):**
   - `load_dir`指定了从哪里加载预训练模型。在这里，它被设置为`"learned_results/stage2/10/models"`。
   - 确认该路径下确实存在预训练模型。如果路径错误或模型不存在，加载操作可能会失败。
   - 当执行增量学习（`add_learning`）、展示（`display`）或评估（`evaluate`）等操作时，这个设置尤为重要。

3. **保存目录 (`save_dir`):**
   - `save_dir`定义了训练后的模型应该保存在哪个目录。这里设置为`"learned_results/stage2"`。
   - 确保指定的保存目录存在且可写。如果目录不存在，程序可能需要创建它，或者会因无法保存模型而出错。
   - 在进行训练操作时（即用户输入`0`），新模型会保存在此目录下。通常会在此目录下创建一个新的子目录（如`/1/models`），其具体名称取决于已有的文件夹结构和命名规则。

### 其他建议

- **路径有效性**: 在开始训练之前，检查`load_dir`和`save_dir`的路径是否正确。无效或不正确的路径可能会导致训练过程中的错误或数据丢失。
- **权限**: 确保您有权限读取`load_dir`中的文件和写入`save_dir`指定的目录。
- **备份**: 在进行大规模更改或长时间训练之前，备份现有的模型和数据是一个好习惯。
- **代码版本控制**: 如果您计划对`train.py`或相关文件进行多次修改，使用版本控制系统（如Git）来管理这些更改会很有帮助。

遵循这些指导原则将有助于确保训练过程的顺利进行，并减少因配置错误而导致的潜在问题。



## 注意事项

- 确保您输入的策略名称与可用策略列表完全匹配。
- 注意保存路径的设置，避免文件管理混乱。

